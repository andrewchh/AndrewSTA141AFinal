---
title: "Neural Predictive Modeling of Trial Outcomes Based on Visual Stimuli in Mice"
author: "Andrew Cao 918156389"
date: "2024-02-25"
output: html_document
---

## Abstract:


This project analyzes a subset data collected by Steinmetz and colleagues. (2019) from 39 sessions of experiments performed on 10 mice. Our analysis focuses on 18 sessions of four mice: Cori Forssman Hence Lederberg. We begin by exploring data structures in the 18 sessions. Then we examine neural activity and changes in each trial, and assess homogeneity or heterogeneity between sessions and mice. To integrate the data using benchmark methods, we extract patterns that are common across sessions. We then build a model using logistic regression based on integrated data. This project is designed to provide insights into the relationship between brain activity and decision-making.


## Section 1 Introduction:


This project is aimed at developing a model to predict accurately the outcome of trials using data on neural activity and visual stimuli. The dataset is a subset from recordings taken during experiments by Steinmetz and colleagues. (2019) on ten mice in 39 sessions. We will analyze 18 sessions of four mice, Cori, Frossman Hence and Lederberg.


In each trial, visual stimuli of varying contrast levels were presented on two screens located next to the mice. The contrast levels were 0, 0.25, 1, or 0, with 0 being no stimulus. These stimuli were used to make decisions by the mice using a wheel that was controlled by their forepaws. The mice were given feedback based on the wheel-turning behaviors.


Spike trains were recorded as the neural activity of the mouse's visual cortex. The spike trains will be examined from the stimulus onset until 0.4 seconds after onset. Key variables include: $feedback type$ : Type of feedback provided to the mice. 1 represents success, and -1 failure. $contrast left$ : Contrast of the left stimulus. $contrast right$ : Contrast of the right stimuli. time : Centers of the time bins used to record spike train data. $spks$ Number of spikes from neurons recorded in the visual cortex for each time bin. brain_area is the area of the cortex where each neuron is situated.


This project will be divided into four part: exploratory analysis,data integration, model training, and predictive modeling. In part 1, we will perform exploratory data analyses to describe data structures across sessions. We will also examine neural activity during trials, explore differences across trials, as well as assess homogeneity or heterogeneity between sessions and mice. This analysis will give us insights into the characteristics of the data and patterns to us to pick the information we interest.


The second part will be devoted to data integration. We will present methods for combining data from different trials. It may be necessary to identify patterns that are common across sessions, or to account for differences in sessions. Our goal is to use the information available from multiple sessions to improve the performance of our prediction model.


In Part 3, we'll develop a model that will accurately predict the outcomes (feedback types of trials). Train set consist 80 percent randomly chosen trials from Session 1 and 18. Test set consist 20 percent randomly chosen trials from Sessions 1 and 18, will be used to evaluate the model's performance. This analysis can have real-world applications, as it will help us understand the relationship between brain activity and decision making processes. The predictive model developed for this project could also be used in other domains such as clinical studies and neuroscience research.


```{r,echo=FALSE}
library(dplyr)
library(ggplot2)
library(readr)
library(tidyverse)
library(caret) 
library(ROCR)
library(pROC)
library(kableExtra)
```


```{r,echo=FALSE}
session=list()
for(i in 1:18){
  session[[i]]=readRDS(paste('./sessions/session',i,'.rds',sep=''))
  #print(session[[i]]$mouse_name)
  
  #print(session[[i]]$date_exp)
  
}
test_session = list()
for (i in 1:2) {
  test_session[[i]]=readRDS(paste('./test/test',i,'.rds',sep=''))
  
}
#  print(test_session[[i]]$mouse_name)
  
#  print(test_session[[i]]$date_exp)
```



##Section 2 Exploratiry Analysis


Within this data set are 18 distinct sessions which feature experiments conducted on four distinct mice. Each session includes a different number of trials. Trials taken place during every session with no missing values being reported and success rate being the ratio between successful trials relative to total trials taken place in any session. It should be noted that this data set is complete, with no missing values for any variable mentioned herein.

```{r,echo=FALSE}
#part 1 (i)
#setting up the structure of the information from the experiments

n.session=length(session) 

# in library tidyverse
meta <- tibble(
  mouse_name = rep('name',n.session),
  date_exp =rep('dt',n.session),
  n_brain_area = rep(0,n.session),
  n_neurons = rep(0,n.session),
  n_trials = rep(0,n.session),
  success_rate = rep(0,n.session)
)

for(i in 1:n.session){
  tmp = session[[i]];
  meta[i,1]=tmp$mouse_name;
  meta[i,2]=tmp$date_exp;
  meta[i,3]=length(unique(tmp$brain_area));
  meta[i,4]=dim(tmp$spks[[1]])[1];
  meta[i,5]=length(tmp$feedback_type);
  meta[i,6]=mean(tmp$feedback_type+1)/2;
}
kable(meta, format = "html", table.attr = "class='table table-striped'",digits=2) 
table_footnote <- "Table 1.Data Structure across Sessions"
```
The data set under consideration contains six key variables: "mouse_name" identifies which mice participated in each session; "date_exp" is when an experiment took place; "n_brain_area" is the count of distinct brain areas examined, while neurons recorded. "n_neurons" depicts the total number of neurons which were observed or measured during an experiment on any given date, or recorded or analyzed during its conduct. It reflects a count of individual nerve cells observed or measured. "n_Trials" displays the total number of trials completed during an experiment. A trial represents either an instance of performing an experiment or repeated measurements under identical conditions. "Success_rate" refers to the proportion of trials that were successful, calculated as the ratio between successful trials and total number of trials; usually expressed either as a percentage or decimal; here it appears as decimals (0.61 represents 61% success rate).

```{r,echo=FALSE}
get_trail_data <- function(session_id, trail_id){
  spikes <- session[[session_id]]$spks[[trail_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trail_tibble <- tibble("neuron_spike" = rowSums(spikes))  %>%  add_column("brain_area" = session[[session_id]]$brain_area ) %>% group_by(brain_area) %>% summarize( region_sum_spike = sum(neuron_spike), region_count = n(),region_mean_spike = mean(neuron_spike)) 
  trail_tibble  = trail_tibble%>% add_column("trail_id" = trail_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trail_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trail_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trail_id])
  trail_tibble
}

```

At our data processing, we examined neural activities recorded during each experimental trial to discover potential predictors of success rate. By analyzing spike data across various brain regions and reviewing trial-specific conditions like visual contrast levels, we attempted to establish which neural patterns may reliably predict successful outcomes. This detailed process involved looking at both sum and mean spike activity as well as visual contrast levels as ways to establish links between neural activity and behavioral performance metrics.
```{r,echo=FALSE}
trail_tibble_1_2 <- get_trail_data(1,2)
trail_tibble_1_2
```
This table presents neural recordings from a neuroscience experiment, showing activity in various brain areas such as the ACA, CA3, and DG. The "region_sum_spike" column tallies the total spikes--which measures neuron action potentials--in each region; "Region_count" represents observations or neurons recorded; "region_mean_spike" shows average spike counts per observation while "Trial id" displays trial ID number while the "contrast left/contrast right" columns may indicate visual contrast in stimuli presented during trials which indicates no contrast (control condition); finally "feedback_type" provides uniform feedback across rows indicating which kind of feedback type (ie "control condition).
```{r, echo=FALSE}

get_session_data <- function(session_id){
  n_trail <- length(session[[session_id]]$spks)
  trail_list <- list()
  for (trail_id in 1:n_trail){
    trail_tibble <- get_trail_data(session_id,trail_id)
    trail_list[[trail_id]] <- trail_tibble
  }
  session_tibble <- do.call(rbind, trail_list)
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}

```
```{r}
session_1 <- get_session_data(1)
head(session_1)
```
```{r}
session_18 <- get_session_data(18)
head(session_18)
```
Benchmark Method 2 analysis has allowed us to identify variables that may impact success rate and brain areas that play a part in it, with particular attention paid to sessions 1-18 and its various trials, respectively. A key challenge has been the wide variance in brain region representation across sessions and trials; therefore requiring us to employ a nuanced approach in order to reconcile any disparate representation and identify patterns contributing to positive results.


```{r,echo=FALSE}
binename <- paste0("bin", as.character(1:40))

get_trail_functional_data <- function(session_id, trail_id){
  spikes <- session[[session_id]]$spks[[trail_id]]
  if (any(is.na(spikes))){
    disp("value missing")
  }

  trail_bin_average <- matrix(colMeans(spikes), nrow = 1)
  colnames(trail_bin_average) <- binename
  trail_tibble  = as_tibble(trail_bin_average)%>% add_column("trail_id" = trail_id) %>% add_column("contrast_left"= session[[session_id]]$contrast_left[trail_id]) %>% add_column("contrast_right"= session[[session_id]]$contrast_right[trail_id]) %>% add_column("feedback_type"= session[[session_id]]$feedback_type[trail_id])
  
  trail_tibble
}
get_session_functional_data <- function(session_id){
  n_trail <- length(session[[session_id]]$spks)
  trail_list <- list()
  for (trail_id in 1:n_trail){
    trail_tibble <- get_trail_functional_data(session_id,trail_id)
    trail_list[[trail_id]] <- trail_tibble
  }
  session_tibble <- as_tibble(do.call(rbind, trail_list))
  session_tibble <- session_tibble %>% add_column("mouse_name" = session[[session_id]]$mouse_name) %>% add_column("date_exp" = session[[session_id]]$date_exp) %>% add_column("session_id" = session_id) 
  session_tibble
}

```
```{r, echo = FALSE}
session_list = list()
for (session_id in 1: 18){
  session_list[[session_id]] <- get_session_functional_data(session_id)
}
```
```{r,echo=FALSE}
full_functional_tibble <- as_tibble(do.call(rbind, session_list))
full_functional_tibble$session_id <- as.factor(full_functional_tibble$session_id )
full_functional_tibble$contrast_diff <- abs(full_functional_tibble$contrast_left-full_functional_tibble$contrast_right)

full_functional_tibble$success <- full_functional_tibble$feedback_type == 1
full_functional_tibble$success <- as.numeric(full_functional_tibble$success)
```
```{r,echo=FALSE}
session_list = list()
for (session_id in 1: 18){
  session_list[[session_id]] <- get_session_data(session_id)
}
full_tibble <- do.call(rbind, session_list)
full_tibble$success <- full_tibble$feedback_type == 1
full_tibble$success <- as.numeric(full_tibble$success)
full_tibble$contrast_diff <- abs(full_tibble$contrast_left-full_tibble$contrast_right)

```

Dotplot showing the distributions of data points over various brain areas during 18 sessions. Each dot represents a measurement, event or occurrence for a particular brain area within a given session. 'brain_area' on the left axis has labels for the different areas of the brain. 'session_id' on the right axis is numbered 1-18.Data points are not distributed uniformly across sessions or areas of the brain. The distribution of data points in some brain areas is not uniform across sessions or brain areas.VISp, VISrl and VISl at the top of the y axis and ACA and ACB near the bottom show data in nearly all sessions. This indicates that these brain areas are consistently active or measured across sessions.ome brain areas are less active or have fewer measurements. The middle section of y-axis shows sparser data in areas like ORBm PIR OLF PL and others. This suggests that these areas are either less active or have fewer measurements.In some sessions, almost all brain regions have a datapoint. For example, sessions 1 and 18 indicate that many brain areas were measured or a lot of activity was taking place.Some sessions like session 8 show fewer data points overall, suggesting that less brain areas were recorded during the session.
```{r,echo=FALSE}
ggplot(full_tibble, aes(x =session_id , y = brain_area)) +
  geom_point() +
  labs(x = "session_id" , y ="brain_area") +
  scale_x_continuous(breaks = unique(full_tibble$session_id)) +  
  theme_minimal()
```

We can observe the following from this data:The numeric values of the brain region columns indicate that the neural activity is different across trials and brain regions.A binary feedback system is used whereby trials are classified as successful ( -1) and unsuccessful ( + 1).Some trials used different contrast levels to the left and the right. This could influence the outcome of the trial or neural activity.
There are 62 levels in brain area
```{r}
area = c()
for(i in 1:n.session){
    tmp = session[[i]];
    area = c(area, unique(tmp$brain_area))
}

area = unique(area)
length(area)
```
From the graph, we are curious about the the total level in brain area. From the program, there are 62 levels in brain area

Subsequently, we began exploring trial-to-trial variations by studying their dynamics; specifically observing interactions between success rate and mean spike activity. The purpose of this phase of research is to understand the intricate relationship between neural response patterns and task performance success.

```{r,echo=FALSE}
full_functional_tibble$trail_group = cut(full_functional_tibble$trail_id, breaks = seq(0, max(full_functional_tibble$trail_id), by = 25),include.lowest = TRUE)
levels(full_functional_tibble$trail_group) <- seq(0, max(full_functional_tibble$trail_id), by = 25)[2:18]
```
```{r,echo=FALSE}
success_rate <- aggregate(success ~ session_id + trail_group, data = full_functional_tibble, FUN = function(x) mean(x) )
ggplot(success_rate, aes(x = trail_group, y = success)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~session_id, ncol=3) +
      theme_bw()

```

The graph shows the success rate for trials in 18 sessions divided into 25 trial bins. Each subplot represents a session and shows the rise and fall of the success rate over the course of the trials. 
```{r,echo=FALSE}
full_functional_tibble %>% group_by(session_id) %>% summarize(success_rate = mean(success, na.rm = TRUE))
```
The above table shows a numerical success rate for each trial. It is easier for us to see the difference between each session. 
```{r,echo=FALSE}
success_rate <- aggregate(success ~ mouse_name + trail_group, data = full_functional_tibble, FUN = function(x) mean(x) )
ggplot(success_rate, aes(x = trail_group, y = success)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~mouse_name) +
      theme_bw()
```

The Graph was divided into four categories: Cori, Forssmann,Hench, and Lederberg.In the graph, initially, a trend upward is seen, suggesting that the mice are improving and adapting to the task. As the trials continue, however, there is a noticeable decline in success rates, which may indicate fatigue or reduced engagement. The variation in the number trials between sessions could be due to the different endurance levels of the mice or their engagement with the experiment.

```{r,echo=FALSE}
full_functional_tibble %>% group_by(mouse_name) %>% summarize(success_rate = mean(success, na.rm = TRUE))
```
The table depicted above quantifies the success rates for individual mice, facilitating an easier observation of variability in performance tolerance among the subjects.
```{r,echo=FALSE}
col_names <-names(full_functional_tibble)
region_sum_subset <- col_names[grep("^region_sum", col_names)]
region_mean_subset <- col_names[grep("^region_mean", col_names)]

```
```{r,echo=FALSE}
# average_spike <- full_tibble %>% group_by( session_id,trail_id) %>% summarise(mean_spike = mean(region_mean_spike))
average_spike <- full_tibble %>% group_by( session_id,trail_id) %>% summarise(mean_spike = sum(region_sum_spike)/sum(region_count))

average_spike$mouse_name <- full_functional_tibble$mouse_name
average_spike$contrast_diff <- full_functional_tibble$contrast_diff
average_spike$success <- full_functional_tibble$success
```
The average_spike is the number of spikes within each number bin divided total number of neurons for each trail. We think that the average spike plays a crucial role in this experiment based on our previous findings. We assume there is a relationship between the average spike and the success rate. The graph below shows the trending of average spike per session. It is indicated by spline. This graph also shows a trend of increasing and then decreasing.
```{r,echo=FALSE}
ggplot(average_spike, aes(x = trail_id, y = mean_spike)) + 
  geom_line()+
  geom_smooth(method = "loess")+  # Fit a smooth spline

  facet_wrap(~session_id)
```

```{r,echo=FALSE}
ggplot(average_spike, aes(x = trail_id, y = mean_spike)) + 
  geom_line()+
  geom_smooth(method = "loess")+  # Fit a smooth spline

  facet_wrap(~mouse_name)
```

"mean_spike" by  Cori, Forssmann,Hench, and Lederberg also shows a same trending with the last graph. This further confirm our assumption

Furthermo, we aimed to determine the average success rate across the entirety of the experiment to gauge overall performance.
```{r}
# feedback_type
n.session=length(session)

n_success = 0
n_trial = 0
for(i in 1:n.session){
    tmp = session[[i]];
    n_trial = n_trial + length(tmp$feedback_type);
    n_success = n_success + sum(tmp$feedback_type == 1);
}
n_success/n_trial
```
From the programming output, we can see over 71 \% trials are success. This indicates that a majority of the trails across all sessions ended in success. 

Summary of Features
feedback_type: Numerous instances indicate success, but potential issues may exist.
contrast_left and contrast_right: Present in four distinct scenarios. In particular,
When left contrast > right contrast, success (1) if turning the wheel to the right and failure (-1) otherwise.
When right contrast > left contrast, success (1) if turning the wheel to the left and failure (-1) otherwise.
When both left and right contrasts are zero, success (1) if holding the wheel still and failure (-1) otherwise.
When left and right contrasts are equal but non-zero, left or right will be randomly chosen (50%) as the correct choice.
mouse_name: Influenced by four factors.
date_exp: May not be directly correlated with success.
brain_area: Comprised of multiple factors; consideration for reduction to a smaller set of factors may be beneficial.
spks: Matrix of dimensions $p_i * q_i$ across session $i$ for number of trials $N_i$.
time:Vector of dimension q across sessions for number of trials $N_i$.


##Section 3 Data Integration


Based on the Benchmark Method, we extract the data from session 18, then combine them into one data.Given that there is only one mouse involved, the mouse_name feature is omitted. The information of contrast_left and contrast_right are summarized into four factors in decision according to the scenrio because a same behavior in different scenrio results in different feedback_type. We employ benchmark method 1, which involves summarizing brain_area, spks, and time into a single number by averaging over spks. 
Then we transform the integrated data into our predictive_feature.
```{r,echo=FALSE}
n_obs = length(session[[18]]$feedback_type)

dat = tibble(
    feedback_type = as.factor(session[[18]]$feedback_type),
    decision = rep('name', n_obs),
    avg_spikes = rep(0, n_obs)
)

for (i in 1:n_obs){
    # decision 
    if (session[[18]]$contrast_left[i] > session[[18]]$contrast_right[i]){
        dat$decision[i] = '1' 
    } else if (session[[18]]$contrast_left[i] < session[[18]]$contrast_right[i]){
        dat$decision[i] = '2' 
    } else if (session[[18]]$contrast_left[i] == session[[18]]$contrast_right[i] 
               & session[[18]]$contrast_left[i] == 0){
        dat$decision[i] = '3' 
    } else{
        dat$decision[i] = '4' 
    }
    
    # avg_spks
    spks.trial = session[[18]]$spks[[i]]
    total.spikes = apply(spks.trial, 1, sum)
    dat$avg_spikes[i] = mean(total.spikes)
}

dat$decision = as.factor(dat$decision)
summary(dat)
```
This R code snippet uses transformed data to examine neural activity. It categorizes decisions based on contrast levels between left and right stimuli and calculates average neural spike counts per trial; four possible outcomes of which correspond with subject responses at various contrast levels are also encoded within its decision logic; this output shows distributions of feedback types for these decisions as well as descriptive statistics of neural spikes recorded - negative feedback being less frequent and 1 and 4 being associated with extreme outcomes - thus the average spike count seems to hover around one, although with some variations across trials; these findings should help uncover some insights into subject responses at various contrast levels as well as analyse neural activity within subjects' responses in different settings.

##Section 4 Predictive modeling

In our predictive model, we choose to use Logistic regression Model to do the prediction.In this section, we apply 3 different models to observe which outputs a most optimal result. Firstly, we apply the model 1 with data from session 18.

```{r}
set.seed(101)
sample <- sample.int(n = n_obs, size = floor(.8 * n_obs), replace = F)
train <- dat[sample, ]
test  <- dat[-sample, ]
fit1 <- glm(feedback_type~., data = train, family="binomial")
summary(fit1)
```
The logistic regression output demonstrates that decision4 and avg_spikes significantly lower, respectively increase, the likelihood of positive feedback type in this model. Decision4 is highly significant with stars representing its strong influence on outcome while intercept and decision2 do not reach statistical significance indicating less direct association. Residual deviance and AIC serve as fit quality measures; former helping demonstrate how well your model explains data while latter aiding comparison between models; both indicators emerge as predictors of success or non-success. avg_spikes emerges as an indicator while decision4 stands as predictors of failure or non-successful action taken against non-successful action taken against this entity (respectively). Overall avg_spikes emerges as predictors whereas decision4 emerge as indicators of non-success.

```{r}
pred1 <- predict(fit1, test %>% select(-feedback_type), type = 'response')
prediction1 <- factor(pred1 > 0.5, labels = c('-1', '1'))
mean(prediction1 != test$feedback_type)
```
In prediction 1, we use the model's predicted probability to make a prediction. The prediction error on the test data set is about 22\%.
```{r,echo=FALSE}
cm <- confusionMatrix(prediction1, test$feedback_type, dnn = c("Prediction", "Reference"))

plt <- as.data.frame(cm$table)

ggplot(plt, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("-1","1")) +
        scale_y_discrete(labels=c("-1","1"))

```



```{r,echo=FALSE}
prediction0 = factor(rep('1', nrow(test)), levels = c('1', '-1'))
mean(prediction0 != test$feedback_type)
```
In prediction 0, the prediction error on the test data is 22\%. This indicates that the prediction is not doing well because if we just bias to success completely, we get same error rate.

```{r,echo=FALSE}
cm <- confusionMatrix(prediction0, test$feedback_type, dnn = c("Prediction", "Reference"))

plt <- as.data.frame(cm$table)

ggplot(plt, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("-1","1")) +
        scale_y_discrete(labels=c("-1","1"))

```
Confusion matrix from biased guess. 


From model2, we apply EDA of the average number of spikes per neuron in each brain area. This summarize "brain_area","spks", and "time", which to average number of spikes per neuron in each brain area. 

```{r,echo=FALSE}
average_spike_area<-function(i.t,this_session){
  spk.trial = this_session$spks[[i.t]]
  area= this_session$brain_area
  spk.count=apply(spk.trial,1,sum)
  spk.average.tapply=tapply(spk.count, area, mean)
  return(spk.average.tapply)
}
n_area = length(unique(session[[18]]$brain_area))
spk_area = matrix(rep(0, n_obs * n_area), n_obs, n_area)
for (i in 1:n_obs){
    spk_area[i,] = average_spike_area(i, session[[18]])
}

spk_area = as_tibble(spk_area)
colnames(spk_area)= unique(session[[18]]$brain_area)
datone = bind_cols(dat, spk_area) %>% select(-avg_spikes)
head(datone)
```

```{r,echo=FALSE}
set.seed(101)

sample <- sample.int(n = n_obs, size = floor(.8 * n_obs), replace = F)
train <- datone[sample, ]
test  <- datone[-sample, ]

fit2 <- glm(feedback_type~., data = train, family="binomial")
summary(fit2)
```
A logistic regression model used several variables to predict feedback_type, with Decision4, ZI and ca03 statistically significant at 0.01 level for their association with feedback_type outcomes; Decision3 reached significance at the 0.05 level and other variables like Intercept decision2, decision2, decision2, CP ACB and TH did not reach significance conventionally; their coefficients indicated impact upon feedback_type being "1", with negative coefficients for Decision4 decreasing its log odds impact while positive ones for ZI and ca03 increasing them; ultimately this model's AIC reached 146.94 after six iterations cycles before convergence was reached.

```{r}
pred2 <- predict(fit2, test %>% select(-feedback_type), type = 'response')
prediction2 <- factor(pred2 > 0.5, labels = c('-1', '1'))
mean(prediction2 != test$feedback_type)

```
In model2, we use the same computation with model 1, but the test data has been trained differently. The prediction error on the test data is 25\%.
```{r,echo=FALSE}
cm <- confusionMatrix(prediction2, test$feedback_type, dnn = c("Prediction", "Reference"))

plt <- as.data.frame(cm$table)

ggplot(plt, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("-1","1")) +
        scale_y_discrete(labels=c("-1","1"))
```
It appears that the model predicts more failures compared to the previous one. However, its performance in predicting failures is not particularly strong.

```{r,echo=FALSE}
# Model 1
pr = prediction(pred1, test$feedback_type)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]

# Model 2
pr = prediction(pred2, test$feedback_type)
prf2 <- performance(pr, measure = "tpr", x.measure = "fpr")
auc2 <- performance(pr, measure = "auc")
auc2 <- auc2@y.values[[1]]

# Bias Guess
pred0 = pred1 * 0 + 1
pr = prediction(pred0, test$feedback_type)
prf0 <- performance(pr, measure = "tpr", x.measure = "fpr")
auc0 <- performance(pr, measure = "auc")
auc0 <- auc0@y.values[[1]]

plot(prf2, ,col = 'red', main = 'ROC curve')
plot(prf, add = TRUE, col = 'blue')
plot(prf0, add = TRUE, col = 'green')
legend("bottomright", legend=c("Model 1", "Model 2", "Bias Guess"), col=c("blue", "red", 'green'), lty=1:1, 
       cex=0.8)
```
ROC curves compare the performance of two predictive models. Both Model 1 and Model 2 exhibit similar abilities in distinguishing between classes, as indicated by their overlapping curves. No single model significantly outperforms either other; both show improved predictive power than random chance (represented by diagonal line). Both true positive rates increase at comparable rates while false positive rates decline with threshold values; suggesting neither model stands out among all others for predictive power within its threshold values range; their closeness to each other in relation to top-left corner indicates balanced sensitivity/specificity balance in both models

```{r}
print(c(auc, auc2, auc0))
```
AUC values show the Area Under the ROC Curve for two models and a baseline guess. Model 2, with an AUC of approximately 0.694, shows superior predictive accuracy than Model 1 with an AUC of approximately 0.659; this measure of ability to distinguish classes indicates which are better classified correctly; additionally, both models outshone random chance which had an AUC value of 0.5; thus showing they outperformed random chance and both may indeed be useful, with Model 2 perhaps being preferable for this specific task.Hence, we will choose model 2 to do prediction performance on the test sets.

##Section 5 Prediction performance on the test sets

In test data set, we pick session 1 from it. We can see the name of the brain area and number of them. 

```{r}
## Test data:
table(test_session[[1]]$brain_area)
```
```{r,echo=FALSE}
n_obstest = length(test_session[[1]]$feedback_type)

dattest = tibble(
    feedback_type = as.factor(test_session[[1]]$feedback_type),
    decision = rep('name', n_obstest),
    avg_spikes = rep(0, n_obstest)
)

for (i in 1:n_obstest){
    # decision 
    if (test_session[[1]]$contrast_left[i] > test_session[[1]]$contrast_right[i]){
        dattest$decision[i] = '1' 
    } else if (test_session[[1]]$contrast_left[i] < test_session[[1]]$contrast_right[i]){
        dattest$decision[i] = '2' 
    } else if (test_session[[1]]$contrast_left[i] == test_session[[1]]$contrast_right[i] 
               & test_session[[1]]$contrast_left[i] == 0){
        dattest$decision[i] = '3' 
    } else{
        dattest$decision[i] = '4' 
    }
    
    # avg_spks
    spks.trial = test_session[[1]]$spks[[i]]
    total.spikes = apply(spks.trial, 1, sum)
    dattest$avg_spikes[i] = mean(total.spikes)
}

dattest$decision = as.factor(dattest$decision)
summary(dattest)
```
```{r,echo=FALSE}
set.seed(101)
sample <- sample.int(n = n_obstest, size = floor(.8 * n_obstest), replace = F)
train <- dattest[sample, ]
test  <- dattest[-sample, ]
fit2test <- glm(feedback_type~., data = train, family="binomial")
```

```{r}
pred2test <-predict(fit2test,test %>% select(-feedback_type),type='response')
numericPredictions <- ifelse(pred2test > 0.5, 1, -1)
levels(test$feedback_type)
prediction2test <- factor(numericPredictions, levels = c(-1, 1))
accuracy <- mean(prediction2test == test$feedback_type)
print(accuracy)


```
From the test data set, by using model2, we receive 70 percent accuracy.
```{r,echo=FALSE}
cm <- confusionMatrix(prediction2test, test$feedback_type, dnn = c("Prediction", "Reference"))

plt <- as.data.frame(cm$table)

ggplot(plt, aes(Reference, Prediction, fill= Freq)) +
        geom_tile() + geom_text(aes(label=Freq)) +
        scale_fill_gradient(low="white", high="#009194") +
        labs(x = "Reference",y = "Prediction") +
        scale_x_discrete(labels=c("-1","1")) +
        scale_y_discrete(labels=c("-1","1"))
```

The confusion matrix displays the performance of a binary classification model. It shows both true labels (Reference), and predicted labels (Prediction), each divided into two categories -1 and 1. More correct predictions were made than incorrect ones as demonstrated by larger numbers along the diagonal cells (12 correct predictions for 1 compared to three correct for -1). Off-diagonal cells display errors (3 instances where true -1 was predicted as 1; two instances where true 1 was misclassified as false), with darker shades denoting higher frequency cases and darker shading representing increased frequencies; thus this model appears more accurately at predicting 1 than its counterpart, suggesting greater accuracy over prediction of false -1 cases.

##Section 6 Discussion

The confusion matrix for the binary classifier shows a 75% accuracy rate, which was same with what we tested from the session data (75percent). From this, we assume the model 2 has a good stability. With both true and predicted labels of -1 and 1, this model demonstrates strong predictive accuracy for 1. Most predictions line up perfectly with actual labels - particularly 1 with 12 correct predictions aligning perfectly - misclassifications are reduced with three instances of misclassified as 1 instances misclassified as -1; two incorrect predictions from this indicate some bias in favor of 1 as opposed to -1 predictions from this model.The model's performance is moderate, we think there are more for improvement.




## Reference {-}

Chatgpt

Course Project Consulting, Milestone II

Course Project demo 1

Course Project demo 2

Steinmetz, N.A., Zatka-Haas, P., Carandini, M. et al. Distributed coding of choice, action and engagement across the mouse brain. Nature 576, 266â€“273 (2019). https://doi.org/10.1038/s41586-019-1787-x

Chen, T., & Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.

Allaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2018. rmarkdown: Dynamic Documents for R. https://rmarkdown.rstudio.com.
